{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "# run in colab\n",
    "# !pip install tensorboard\n",
    "# !rm -rvf logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "\n",
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.\n",
    "m = X_train.shape[0]\n",
    "print(X_train[0])\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01]))\n",
    "HP_INITIALIZER = hp.HParam('initializer', hp.Discrete(['uniform', 'random_normal']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0., 0.2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['SGD', 'Adam']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'tanh']))\n",
    "\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_ACTIVATION, HP_DROPOUT, HP_OPTIMIZER, HP_LEARNING_RATE, HP_INITIALIZER],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)\n",
    "    \n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "def create_model(hparams, nodes=[300,100], batch_normalization=True, epochs_num=20, batch_size=32):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    for node in nodes:\n",
    "        if batch_normalization:\n",
    "          model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(node, activation=hparams[HP_ACTIVATION], kernel_initializer = hparams[HP_INITIALIZER]))\n",
    "        model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT]))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "    if hparams[HP_OPTIMIZER] == 'Adam':\n",
    "      optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "    elif hparams[HP_OPTIMIZER] == 'SGD':\n",
    "      optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "    model.fit(X_train, \n",
    "              y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs_num,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[early_stopping_cb])\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def experiment(experiment_dir, hparams):\n",
    "\n",
    "    with tf.summary.create_file_writer(experiment_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        accuracy = create_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "experiment_no = 0\n",
    "\n",
    "for initializer in HP_INITIALIZER.domain.values:\n",
    "  for activation in HP_ACTIVATION.domain.values:\n",
    "      for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "          for optimizer in HP_OPTIMIZER.domain.values:\n",
    "              for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                  hparams = {\n",
    "                      HP_ACTIVATION: activation,\n",
    "                      HP_DROPOUT: dropout_rate,\n",
    "                      HP_OPTIMIZER: optimizer,\n",
    "                      HP_LEARNING_RATE: learning_rate,\n",
    "                      HP_INITIALIZER: initializer\n",
    "                  }\n",
    "\n",
    "\n",
    "              experiment_name = f'Experiment {experiment_no}'\n",
    "              print(f'Starting Experiment: {experiment_name}')\n",
    "              print({h.name: hparams[h] for h in hparams})\n",
    "              experiment('logs/hparam_tuning/' + experiment_name, hparams)\n",
    "              experiment_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir logs/hparam_tuning --reload_interval 15\n",
    "# Launches tensorboard to read the logs from ./logs directory that was specified earlier when creating the callback\n",
    "%tensorboard --logdir my_logs --reload_interval 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning NN Hyperparameters with keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in colab\n",
    "%pip install -q -U keras-tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
    "                             sampling=\"log\")\n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "random_search_tuner = kt.RandomSearch(\n",
    "    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n",
    "    directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)\n",
    "random_search_tuner.search(X_train, y_train, epochs=10,\n",
    "                           validation_data=(X_valid, y_valid))\n",
    "\n",
    "top3_models = random_search_tuner.get_best_models(num_models=3)\n",
    "best_model = top3_models[0]\n",
    "\n",
    "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_trial.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist\n",
    "X_train_full = np.expand_dims(X_train_full, axis=-1).astype(np.float32) / 255\n",
    "X_test = np.expand_dims(X_test.astype(np.float32), axis=-1) / 255\n",
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01]))\n",
    "HP_INITIALIZER = hp.HParam('initializer', hp.Discrete(['he_normal','random_normal', 'uniform']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0., 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['SGD','Adam']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu','tanh']))\n",
    "HP_KERNEL_SIZE = hp.HParam('kernel_size', hp.Discrete([3]))\n",
    "HP_PADDING = hp.HParam('padding', hp.Discrete(['same', 'valid']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logsCnn/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_ACTIVATION, HP_DROPOUT, HP_OPTIMIZER, HP_LEARNING_RATE, HP_INITIALIZER, HP_KERNEL_SIZE, HP_PADDING],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def create_cnn_model(hparams, filtersList=[64,128, 256], nodes=[300,100], batch_normalization=True, epochs_num=20, batch_size=32):\n",
    "    DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding='same',\n",
    "                        activation=hparams[HP_ACTIVATION], kernel_initializer='he_normal')\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    for filters in filtersList:\n",
    "      model.add(DefaultConv2D(filters=filters, input_shape=[28, 28, 1]))\n",
    "      model.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for node in nodes:\n",
    "        model.add(tf.keras.layers.Dense(node, activation=hparams[HP_ACTIVATION], kernel_initializer = hparams[HP_INITIALIZER]))\n",
    "        model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT]))\n",
    "        if batch_normalization:\n",
    "          model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "    if hparams[HP_OPTIMIZER] == 'Adam':\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "    elif hparams[HP_OPTIMIZER] == 'SGD':\n",
    "      optimizer = tf.keras.optimizers.SGD(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs_num,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[early_stopping_cb])\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def experiment(experiment_dir, hparams):\n",
    "\n",
    "    with tf.summary.create_file_writer(experiment_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        accuracy = create_cnn_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "experiment_no = 0\n",
    "\n",
    "for initializer in HP_INITIALIZER.domain.values:\n",
    "  for activation in HP_ACTIVATION.domain.values:\n",
    "      for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "          for optimizer in HP_OPTIMIZER.domain.values:\n",
    "              for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                  hparams = {\n",
    "                      HP_ACTIVATION: activation,\n",
    "                      HP_DROPOUT: dropout_rate,\n",
    "                      HP_OPTIMIZER: optimizer,\n",
    "                      HP_LEARNING_RATE: learning_rate,\n",
    "                      HP_INITIALIZER: initializer\n",
    "                  }\n",
    "\n",
    "\n",
    "              experiment_name = f'Experiment {experiment_no}'\n",
    "              print(f'Starting Experiment: {experiment_name}')\n",
    "              print({h.name: hparams[h] for h in hparams})\n",
    "              experiment('logs/hparam_tuning/' + experiment_name, hparams)\n",
    "              experiment_no += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning CNN Hyperparameters with keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q -U keras-tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):  \n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
    "        kernel_size=hp.Choice('conv_1_kernel', values = [3,5]),\n",
    "        activation='relu',\n",
    "        input_shape=(28,28,1)\n",
    "    ),\n",
    "    tf.keras.layers.Conv2D(\n",
    "        filters=hp.Int('conv_2_filter', min_value=32, max_value=64, step=16),\n",
    "        kernel_size=hp.Choice('conv_2_kernel', values = [3,5]),\n",
    "        activation='relu'\n",
    "    ),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(\n",
    "        units=hp.Int('dense_1_units', min_value=32, max_value=128, step=16),\n",
    "        activation='relu'\n",
    "    ),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "  \n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "  \n",
    "  return model\n",
    "\n",
    "random_search_tuner=kt.RandomSearch(build_model,objective='val_accuracy',max_trials=5,directory='output',project_name=\"Mnist Fashion\")\n",
    "\n",
    "random_search_tuner.search(X_train, y_train, epochs=10,\n",
    "                           validation_data=(X_valid, y_valid))\n",
    "\n",
    "top3_models = random_search_tuner.get_best_models(num_models=3)\n",
    "best_model = top3_models[0]\n",
    "\n",
    "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_trial.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(top3_models)):\n",
    "    print(f'Model {i+1}:')\n",
    "    print(top3_models[i].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best accuaracy\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "\n",
    "conv_net = tf.keras.Sequential()\n",
    "conv_net.add(tf.keras.layers.Input(shape=[28, 28, 1]))\n",
    "\n",
    "conv_net.add(tf.keras.layers.BatchNormalization())\n",
    "conv_net.add(tf.keras.layers.Conv2D(filters=32, kernel_size=5, activation='relu'))\n",
    "conv_net.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "conv_net.add(tf.keras.layers.BatchNormalization())\n",
    "conv_net.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
    "conv_net.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "conv_net.add(tf.keras.layers.BatchNormalization())\n",
    "conv_net.add(tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation='relu'))\n",
    "conv_net.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "conv_net.add(tf.keras.layers.Flatten())\n",
    "conv_net.add(tf.keras.layers.BatchNormalization())\n",
    "conv_net.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "\n",
    "conv_net.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "loss = tf.keras.losses.sparse_categorical_crossentropy\n",
    "conv_net.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "\n",
    "conv_net.fit(X_train, y_train, epochs=50, validation_data=(X_valid, y_valid))\n",
    "\n",
    "conv_net.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
