{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from functools import partial\n",
    "import pandas as pd \n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorboard.plugins.hparams import api as hp\n",
    "# run in colab\n",
    "# !pip install tensorboard\n",
    "# !rm -rvf logs\n",
    "\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist\n",
    "X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]\n",
    "X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]\n",
    "\n",
    "X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.\n",
    "m = X_train.shape[0]\n",
    "\n",
    "class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01]))\n",
    "HP_INITIALIZER = hp.HParam('initializer', hp.Discrete(['uniform', 'random_normal']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0., 0.2))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['SGD', 'Adam']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu', 'tanh']))\n",
    "\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_ACTIVATION, HP_DROPOUT, HP_OPTIMIZER, HP_LEARNING_RATE, HP_INITIALIZER],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)\n",
    "    \n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "def create_model(hparams, nodes=[300,100], batch_normalization=True, epochs_num=20, batch_size=32):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=(28, 28)))\n",
    "\n",
    "    for node in nodes:\n",
    "        if batch_normalization:\n",
    "          model.add(tf.keras.layers.BatchNormalization())\n",
    "        model.add(tf.keras.layers.Dense(node, activation=hparams[HP_ACTIVATION], kernel_initializer = hparams[HP_INITIALIZER]))\n",
    "        model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT]))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "    if hparams[HP_OPTIMIZER] == 'Adam':\n",
    "      optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "    elif hparams[HP_OPTIMIZER] == 'SGD':\n",
    "      optimizer = tf.keras.optimizers.legacy.SGD(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "    model.fit(X_train, \n",
    "              y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs_num,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[early_stopping_cb])\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def experiment(experiment_dir, hparams):\n",
    "\n",
    "    with tf.summary.create_file_writer(experiment_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        accuracy = create_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "experiment_no = 0\n",
    "\n",
    "for initializer in HP_INITIALIZER.domain.values:\n",
    "  for activation in HP_ACTIVATION.domain.values:\n",
    "      for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "          for optimizer in HP_OPTIMIZER.domain.values:\n",
    "              for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                  hparams = {\n",
    "                      HP_ACTIVATION: activation,\n",
    "                      HP_DROPOUT: dropout_rate,\n",
    "                      HP_OPTIMIZER: optimizer,\n",
    "                      HP_LEARNING_RATE: learning_rate,\n",
    "                      HP_INITIALIZER: initializer\n",
    "                  }\n",
    "\n",
    "\n",
    "              experiment_name = f'Experiment {experiment_no}'\n",
    "              print(f'Starting Experiment: {experiment_name}')\n",
    "              print({h.name: hparams[h] for h in hparams})\n",
    "              experiment('logs/hparam_tuning/' + experiment_name, hparams)\n",
    "              experiment_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "\n",
    "# %tensorboard --logdir logs/hparam_tuning --reload_interval 15\n",
    "# Launches tensorboard to read the logs from ./logs directory that was specified earlier when creating the callback\n",
    "%tensorboard --logdir my_logs --reload_interval 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-Tuning NN Hyperparameters with keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in colab\n",
    "# %pip install -q -U keras-tuner\n",
    "# import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    n_hidden = hp.Int(\"n_hidden\", min_value=0, max_value=8, default=2)\n",
    "    n_neurons = hp.Int(\"n_neurons\", min_value=16, max_value=256)\n",
    "    learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2,\n",
    "                             sampling=\"log\")\n",
    "    optimizer = hp.Choice(\"optimizer\", values=[\"sgd\", \"adam\"])\n",
    "    if optimizer == \"sgd\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for _ in range(n_hidden):\n",
    "        model.add(tf.keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
    "    model.add(tf.keras.layers.Dense(10, activation=\"softmax\"))\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "                  metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "random_search_tuner = kt.RandomSearch(\n",
    "    build_model, objective=\"val_accuracy\", max_trials=5, overwrite=True,\n",
    "    directory=\"my_fashion_mnist\", project_name=\"my_rnd_search\", seed=42)\n",
    "random_search_tuner.search(X_train, y_train, epochs=10,\n",
    "                           validation_data=(X_valid, y_valid))\n",
    "\n",
    "top3_models = random_search_tuner.get_best_models(num_models=3)\n",
    "best_model = top3_models[0]\n",
    "\n",
    "best_trial = random_search_tuner.oracle.get_best_trials(num_trials=1)[0]\n",
    "best_trial.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment: Experiment 0\n",
      "{'activation': 'relu', 'dropout': 0.0, 'optimizer': 'SGD', 'learning_rate': 0.01, 'initializer': 'he_normal'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 11:25:26.226518: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node SGD/AssignVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 40s 20ms/step - loss: 0.4845 - accuracy: 0.8302 - val_loss: 0.4121 - val_accuracy: 0.8584\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.3561 - accuracy: 0.8741 - val_loss: 0.3383 - val_accuracy: 0.8768\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.3207 - accuracy: 0.8853 - val_loss: 0.3151 - val_accuracy: 0.8890\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2963 - accuracy: 0.8955 - val_loss: 0.3335 - val_accuracy: 0.8838\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.2817 - accuracy: 0.8986 - val_loss: 0.3530 - val_accuracy: 0.8708\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.2698 - accuracy: 0.9050 - val_loss: 0.3434 - val_accuracy: 0.8804\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.2626 - accuracy: 0.9057 - val_loss: 0.2876 - val_accuracy: 0.8968\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2585 - accuracy: 0.9091 - val_loss: 0.2860 - val_accuracy: 0.9006\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2581 - accuracy: 0.9090 - val_loss: 0.3530 - val_accuracy: 0.8748\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2580 - accuracy: 0.9103 - val_loss: 0.3619 - val_accuracy: 0.8890\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2724 - accuracy: 0.9085 - val_loss: 0.4418 - val_accuracy: 0.8648\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.2962 - accuracy: 0.9037 - val_loss: 0.6221 - val_accuracy: 0.8592\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.3553 - accuracy: 0.8945 - val_loss: 0.4030 - val_accuracy: 0.8860\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.4432 - accuracy: 0.8887 - val_loss: 0.7493 - val_accuracy: 0.8522\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.6370 - accuracy: 0.8777 - val_loss: 0.8213 - val_accuracy: 0.8738\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 0.8976 - accuracy: 0.8672 - val_loss: 3.6321 - val_accuracy: 0.7216\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 1.3020 - accuracy: 0.8548 - val_loss: 3.6684 - val_accuracy: 0.8066\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 30s 17ms/step - loss: 2.3043 - accuracy: 0.8359 - val_loss: 2.2552 - val_accuracy: 0.7700\n",
      "313/313 [==============================] - 2s 8ms/step - loss: 0.3145 - accuracy: 0.8943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Experiment: Experiment 1\n",
      "{'activation': 'relu', 'dropout': 0.5, 'optimizer': 'SGD', 'learning_rate': 0.01, 'initializer': 'he_normal'}\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-13 11:34:40.810316: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:961] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node SGD/AssignVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1719/1719 [==============================] - 41s 21ms/step - loss: 0.7487 - accuracy: 0.7438 - val_loss: 0.4322 - val_accuracy: 0.8424\n",
      "Epoch 2/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.4949 - accuracy: 0.8246 - val_loss: 0.3972 - val_accuracy: 0.8538\n",
      "Epoch 3/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.4362 - accuracy: 0.8467 - val_loss: 0.3398 - val_accuracy: 0.8786\n",
      "Epoch 4/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.3976 - accuracy: 0.8607 - val_loss: 0.3234 - val_accuracy: 0.8794\n",
      "Epoch 5/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.3730 - accuracy: 0.8675 - val_loss: 0.3259 - val_accuracy: 0.8784\n",
      "Epoch 6/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.3552 - accuracy: 0.8756 - val_loss: 0.3017 - val_accuracy: 0.8894\n",
      "Epoch 7/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.3389 - accuracy: 0.8796 - val_loss: 0.3369 - val_accuracy: 0.8746\n",
      "Epoch 8/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.3300 - accuracy: 0.8848 - val_loss: 0.2800 - val_accuracy: 0.8950\n",
      "Epoch 9/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.3164 - accuracy: 0.8889 - val_loss: 0.3376 - val_accuracy: 0.8746\n",
      "Epoch 10/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.3041 - accuracy: 0.8947 - val_loss: 0.2869 - val_accuracy: 0.8956\n",
      "Epoch 11/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.3022 - accuracy: 0.8947 - val_loss: 0.2680 - val_accuracy: 0.9024\n",
      "Epoch 12/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.2888 - accuracy: 0.8987 - val_loss: 0.2692 - val_accuracy: 0.9006\n",
      "Epoch 13/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.2805 - accuracy: 0.9011 - val_loss: 0.2624 - val_accuracy: 0.9026\n",
      "Epoch 14/20\n",
      "1719/1719 [==============================] - 30s 18ms/step - loss: 0.2736 - accuracy: 0.9042 - val_loss: 0.2756 - val_accuracy: 0.9024\n",
      "Epoch 15/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2654 - accuracy: 0.9054 - val_loss: 0.2661 - val_accuracy: 0.9036\n",
      "Epoch 16/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2600 - accuracy: 0.9090 - val_loss: 0.2752 - val_accuracy: 0.9014\n",
      "Epoch 17/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2555 - accuracy: 0.9093 - val_loss: 0.2732 - val_accuracy: 0.8970\n",
      "Epoch 18/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2475 - accuracy: 0.9120 - val_loss: 0.2575 - val_accuracy: 0.9076\n",
      "Epoch 19/20\n",
      "1719/1719 [==============================] - 32s 18ms/step - loss: 0.2455 - accuracy: 0.9138 - val_loss: 0.2522 - val_accuracy: 0.9094\n",
      "Epoch 20/20\n",
      "1719/1719 [==============================] - 31s 18ms/step - loss: 0.2374 - accuracy: 0.9171 - val_loss: 0.2491 - val_accuracy: 0.9122\n",
      "313/313 [==============================] - 3s 8ms/step - loss: 0.2699 - accuracy: 0.9027\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "\n",
    "early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "\n",
    "HP_LEARNING_RATE = hp.HParam('learning_rate', hp.Discrete([0.001, 0.01]))\n",
    "HP_INITIALIZER = hp.HParam('initializer', hp.Discrete(['he_normal','random_normal', 'uniform']))\n",
    "HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0., 0.5))\n",
    "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['SGD','Adam']))\n",
    "HP_ACTIVATION = hp.HParam('activation', hp.Discrete(['relu','tanh']))\n",
    "HP_KERNEL_SIZE = hp.HParam('kernel_size', hp.Discrete([3]))\n",
    "HP_PADDING = hp.HParam('padding', hp.Discrete(['same', 'valid']))\n",
    "\n",
    "METRIC_ACCURACY = 'accuracy'\n",
    "\n",
    "with tf.summary.create_file_writer('logsCnn/hparam_tuning').as_default():\n",
    "    hp.hparams_config(\n",
    "        hparams=[HP_ACTIVATION, HP_DROPOUT, HP_OPTIMIZER, HP_LEARNING_RATE, HP_INITIALIZER, HP_KERNEL_SIZE, HP_PADDING],\n",
    "        metrics=[hp.Metric(METRIC_ACCURACY, display_name='Accuracy')],)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "def create_cnn_model(hparams, filtersList=[64,128, 256], nodes=[300,100], batch_normalization=True, epochs_num=20, batch_size=32):\n",
    "    DefaultConv2D = partial(tf.keras.layers.Conv2D, kernel_size=3, padding='same',\n",
    "                        activation=hparams[HP_ACTIVATION], kernel_initializer='he_normal')\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    for filters in filtersList:\n",
    "      model.add(DefaultConv2D(filters=filters, input_shape=[28, 28, 1]))\n",
    "      model.add(tf.keras.layers.MaxPool2D())\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    for node in nodes:\n",
    "        model.add(tf.keras.layers.Dense(node, activation=hparams[HP_ACTIVATION], kernel_initializer = hparams[HP_INITIALIZER]))\n",
    "        model.add(tf.keras.layers.Dropout(hparams[HP_DROPOUT]))\n",
    "        if batch_normalization:\n",
    "          model.add(tf.keras.layers.BatchNormalization())\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "\n",
    "    if hparams[HP_OPTIMIZER] == 'Adam':\n",
    "      optimizer = tf.keras.optimizers.Adam(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "    elif hparams[HP_OPTIMIZER] == 'SGD':\n",
    "      optimizer = tf.keras.optimizers.SGD(learning_rate=hparams[HP_LEARNING_RATE])\n",
    "\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(patience=10,\n",
    "                                                     restore_best_weights=True)\n",
    "    model.fit(X_train,\n",
    "              y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs_num,\n",
    "              validation_data=(X_valid, y_valid),\n",
    "              callbacks=[early_stopping_cb])\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def experiment(experiment_dir, hparams):\n",
    "\n",
    "    with tf.summary.create_file_writer(experiment_dir).as_default():\n",
    "        hp.hparams(hparams)\n",
    "        accuracy = create_cnn_model(hparams)\n",
    "        tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "experiment_no = 0\n",
    "\n",
    "for initializer in HP_INITIALIZER.domain.values:\n",
    "  for activation in HP_ACTIVATION.domain.values:\n",
    "      for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):\n",
    "          for optimizer in HP_OPTIMIZER.domain.values:\n",
    "              for learning_rate in HP_LEARNING_RATE.domain.values:\n",
    "                  hparams = {\n",
    "                      HP_ACTIVATION: activation,\n",
    "                      HP_DROPOUT: dropout_rate,\n",
    "                      HP_OPTIMIZER: optimizer,\n",
    "                      HP_LEARNING_RATE: learning_rate,\n",
    "                      HP_INITIALIZER: initializer\n",
    "                  }\n",
    "\n",
    "\n",
    "              experiment_name = f'Experiment {experiment_no}'\n",
    "              print(f'Starting Experiment: {experiment_name}')\n",
    "              print({h.name: hparams[h] for h in hparams})\n",
    "              experiment('logs/hparam_tuning/' + experiment_name, hparams)\n",
    "              experiment_no += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-metal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
